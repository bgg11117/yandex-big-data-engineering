{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will use Apache Spark to build simple movie recommendation system on big MovieLens 20M dataset (https://grouplens.org/datasets/movielens/). The dataset contains 20 million ratings for 27000 movies given by 138000 users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset format: comma-separated values (https://en.wikipedia.org/wiki/CSV)\n",
    "\n",
    "The first line is a header:\n",
    "userId,movieId,rating,timestamp\n",
    "\n",
    "_userId_, _movieId_ are integers representing user and movies identifiers correspondingly\n",
    "\n",
    "_rating_ is a floating point number from 1.0 to 5.0\n",
    "\n",
    "_timestamp_ is an integer but it wonâ€™t be used in the assignment :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .appName(\"recSys\")\\\n",
    "    .master(\"local[8]\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use spark session or spark context to read csv.\n",
    "def skip_header(rdd):\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda row: row != header)\n",
    "    return rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib import recommendation\n",
    "from pyspark import SparkContext\n",
    "def to_rating(ml_latest_row):\n",
    "    user, movie, rating, timestamp = ml_latest_row.split(',')\n",
    "    return recommendation.Rating(int(user), int(movie), float(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.textFile(\"/data/movielens/ratings_100k.csv\")\n",
    "rdd = skip_header(rdd)\n",
    "rdd = rdd.map(to_rating).repartition(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will use collaborative filtering approach for making predictions. To find latent vector representation for users and items we suggest you to use explicit ALS method. Refer to the Spark MLLib documentation for Python API details: https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into three folds: on each iteration one fold will be used for testing and the other two folds will be used for training. Wrap necessary values with _Rating_ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "\n",
    "fold_count = 3\n",
    "train1, train2,train3 = rdd.randomSplit(weights=[1,1,1], seed=123)\n",
    "val=[(train1.union(train2),train3),\n",
    "      (train3.union(train1),train2),\n",
    "       (train3.union(train2),train1)]\n",
    "val_cnt=[ (x.count(),y.count()) for x,y in val ]\n",
    "tot=[x+y for x,y in val_cnt ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations: to make your solution more efficient prepare RDDs for all the folds beforehand running the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ALS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to choose training parameters. It is recommended to set the rank equal to 20,\n",
    "regularization term lambda equal to 0.01 and the number of iterations equal to 5.\n",
    "Feel free to experiment and choose your own parameters to see how they influence the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 2\n",
    "iterations =5\n",
    "lambda_ = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the explicit ALS model on two of three folds and evaluate its performance on remaining test fold. Performance evaluation should be done using the classic RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRmse(model, data, n):\n",
    "    \"\"\"\n",
    "    Compute RMSE (Root Mean Squared Error).\n",
    "    \"\"\"\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    predictions = model.predictAll(data.map(lambda x: (x[0], x[1])))\n",
    "    predictionsAndRatings = predictions.map(lambda x: ((x[0], x[1]), x[2])) \\\n",
    "      .join(data.map(lambda x: ((x[0], x[1]), x[2]))) \\\n",
    "      .values()\n",
    "    return sqrt(predictionsAndRatings.map(lambda x: (x[0] - x[1]) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 3\n",
    "for test_fold_index in xrange(fold_count):\n",
    "    als = ALS.train(val[test_fold_index][0],rank,iterations,lambda_)\n",
    "    scores[test_fold_index]=computeRmse(als,val[test_fold_index][1],val_cnt[test_fold_index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average RMSE scores on all the three folds and output the result to stdout.\n",
    "You could refer to publicly available benchmarks (e.g. http://mymedialite.net/examples/datasets.html)\n",
    "to find out what score to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916640441496\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
